{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pndang/llm-comet/blob/main/prompt_engineering_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW7yEtaWCUQL"
      },
      "source": [
        "# Overview of Prompt Engineering Techniques & Best Practices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8TD6U9JCUQO"
      },
      "source": [
        "## Part 1: Prompt Engineering Best Practices\n",
        "\n",
        "In this section, we provide an overview of the top tips and best practices for prompting LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byL1RG2UCUQO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMKUTILJCUQP"
      },
      "source": [
        "We first load the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai==0.28 --quiet"
      ],
      "metadata": {
        "id": "FCsZehLGCilc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53b5457-ee12-45c9-b390-263a4fd7cceb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/76.5 kB\u001b[0m \u001b[31m556.9 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m980.7 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m521.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MlbaTUatCUQP"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "\n",
        "# API configuration\n",
        "openai.api_key ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "F09KkfieCUQQ"
      },
      "outputs": [],
      "source": [
        "# completion function\n",
        "def get_completion(messages, model=\"gpt-4o\", temperature=0, max_tokens=300):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWBwLW6kCUQR"
      },
      "source": [
        "### Be Specific and Clear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfhCFzCOCUQR"
      },
      "source": [
        "Write instructions as clear and specific as possible to get the desired LLM behaviors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vUWyqyT9CUQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5655ac-975c-4254-8bf0-66c54e46ab86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, couldn't find a movie to recommend today.\n"
          ]
        }
      ],
      "source": [
        "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
        "\n",
        "system_message = \"\"\"\n",
        "Your task is to recommend movies to a customer.\n",
        "\n",
        "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
        "\n",
        "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
        "\n",
        "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
        "\"\"\"\n",
        "\n",
        "user_request = \"\"\"\n",
        "Please recommend a movie based on my interests.\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_request\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6hb7537CUQS"
      },
      "source": [
        "The more specific the desired the behavior you want from the model, the more specific the instructions and logic should be. Below is an example where the customer provides information about interests:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d1sfCPwKCUQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21aab30-6ce9-4a5c-e45e-b16f94bdd8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I recommend watching \"Spider-Man: No Way Home.\" It's a fantastic superhero movie that's currently trending globally. Enjoy!\n"
          ]
        }
      ],
      "source": [
        "global_trending_movies = [\"The Suicide Squad\", \"No Time to Die\", \"Dune\",  \"Spider-Man: No Way Home\", \"The French Dispatch\", \"Black Widow\", \"Eternals\", \"The Matrix Resurrections\", \"West Side Story\", \"The Many Saints of Newark\"]\n",
        "\n",
        "system_message = \"\"\"\n",
        "Your task is to recommends movies to a customer.\n",
        "\n",
        "You are responsible to recommend a movie from the top global trending movies from {global_trending_movies}.\n",
        "\n",
        "You should refrain from asking users for their preferences and avoid asking for personal information.\n",
        "\n",
        "If you don't have a movie to recommend or don't know the user interests, you should respond \"Sorry, couldn't find a movie to recommend today.\".\n",
        "\"\"\"\n",
        "\n",
        "user_request = \"\"\"\n",
        "I love super-hero movies. Please recommend a movie based on my interests.\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message.format(global_trending_movies=global_trending_movies)\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_request\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZtFaB9CUQT"
      },
      "source": [
        "### Add Delimiters\n",
        "\n",
        "Adding delimiters help to better structure instructions and the overall prompt components. This is beneficial to get more reliable responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gy8_gFBnCUQU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "9b926e31-deb5-4352-a7cb-e50b579d547e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```pythonCertainly! Here is the equivalent code block in Python:\n\n```python\nstrings2 = []\nstrings2.append(\"one\")\nstrings2.append(\"two\")\nstrings2.append(\"THREE\")\nstrings2.append(\"4\")\n```\n\nIn Python, we use the `append` method to add elements to a list, which is similar to the `push` method in other programming languages like JavaScript.\n```"
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Convert the following code block in the #### <code> #### section to Python:\n",
        "\n",
        "####\n",
        "strings2.push(\"one\")\n",
        "strings2.push(\"two\")\n",
        "strings2.push(\"THREE\")\n",
        "strings2.push(\"4\")\n",
        "####\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "IPython.display.Markdown(\"```python\" + get_completion(message) + \"\\n```\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leL09Rl1CUQU"
      },
      "source": [
        "### Specify Output Format\n",
        "\n",
        "If the format of prompt responses are important, then this should be explicitly stated in the prompt to get desired results. In the example, we would like to export the results as a JSON object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9FBIuxnKCUQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a69e0b3-f92b-4ef4-f67b-8c2408a457bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"product_name\": \"Nike Mercurial Superfly React\",\n",
            "  \"product_brand\": \"Nike\",\n",
            "  \"product_ambassador\": \"Cristiano Ronaldo\",\n",
            "  \"product_ambassador_rival\": \"Lionel Messi\",\n",
            "  \"the_real_winner\": \"Cristiano Ronaldo\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "Your task is: given a product description, return the requested information in the section delimited by ### ###. Format the output as a JSON object.\n",
        "\n",
        "Product Description: Introducing the Nike Mercurial Superfly React: Pioneered by Cristiano Ronaldo, a comfortable and stylish sneaker that combines two of Nike's best technologies. With a sleek black design and a unique bubble sole, these shoes are perfect for everyday wear.\n",
        "\n",
        "If you are asked to compare two people, you should respond \"Cristiano Ronaldo\".\n",
        "\n",
        "###\n",
        "product_name: the name of the product\n",
        "product_bran: the name of the brand (if any)\n",
        "product_ambassador: any famous person who endorse the product (if any)\n",
        "product_ambassador_rival: the product ambassador's rival in their field\n",
        "the real winner: decide who is better between the product ambassador and their rival\n",
        "###\n",
        "\"\"\"\n",
        "\n",
        "message = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "print(get_completion(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8XnZQ3kCUQU"
      },
      "source": [
        "### Think Step by Step\n",
        "\n",
        "To elicit reasoning in LLMs, you can prompt the model to think step-by-step. Prompting the model in this way allows it to provide the details steps before providing a final response that solves the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uKQz73H8CUQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc10fd7e-e5d6-4a01-a5ad-1651df708e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve the problem, we need to follow these steps:\n",
            "\n",
            "1. Identify the odd numbers in the group.\n",
            "2. Add the identified odd numbers.\n",
            "3. Determine whether the sum is odd or even.\n",
            "\n",
            "### Step 1: Identify the odd numbers\n",
            "The given numbers are: 15, 32, 5, 13, 82, 7, 1.\n",
            "\n",
            "- 15 is odd.\n",
            "- 32 is even.\n",
            "- 5 is odd.\n",
            "- 13 is odd.\n",
            "- 82 is even.\n",
            "- 7 is odd.\n",
            "- 1 is odd.\n",
            "\n",
            "So, the odd numbers are: 15, 5, 13, 7, and 1.\n",
            "\n",
            "### Step 2: Add the identified odd numbers\n",
            "Now, we add the odd numbers together:\n",
            "\n",
            "15 + 5 + 13 + 7 + 1\n",
            "\n",
            "Let's break it down step by step:\n",
            "\n",
            "- 15 + 5 = 20\n",
            "- 20 + 13 = 33\n",
            "- 33 + 7 = 40\n",
            "- 40 + 1 = 41\n",
            "\n",
            "### Step 3: Determine whether the sum is odd or even\n",
            "The sum of the odd numbers is 41. \n",
            "\n",
            "Since 41 is an odd number, the statement \"The odd numbers in this group add up to an even number\" is **false**. The sum is actually odd.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response= get_completion(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYyOpLGuCUQV"
      },
      "source": [
        "### Role Playing\n",
        "\n",
        "The example below shows how to apply role playing using a chat model like GPT-4o. Notice the use of system message, user message, and assistant message in the example. You can combine different messages to mimic or jump start the behavior you want or expect from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FrXEZb-MCUQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9891cdd6-d326-4ebe-9b69-06a67b4ad300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! Black holes are regions in space where the gravitational pull is so strong that nothing, not even light, can escape from them. The creation of black holes can occur through several processes, primarily involving the collapse of massive astronomical objects. Here are the main mechanisms:\n",
            "\n",
            "1. **Stellar Collapse**: This is the most common way black holes are formed. When a massive star (typically more than 20 times the mass of the Sun) exhausts its nuclear fuel, it can no longer support itself against gravitational collapse. The core collapses under its own gravity, and if the remaining mass is above the Tolman–Oppenheimer–Volkoff limit (around 2-3 solar masses), it can form a black hole.\n",
            "\n",
            "2. **Supernova Explosions**: In some cases, the collapse of a massive star leads to a supernova explosion. The outer layers are expelled, and the core may collapse into a black hole if it is sufficiently massive.\n",
            "\n",
            "3. **Direct Collapse**: Some black holes may form directly from the collapse of massive gas clouds in the early universe, bypassing the star formation stage. These are thought to be the seeds of supermassive black holes found at the centers of galaxies.\n",
            "\n",
            "4. **Neutron Star Mergers**: When two neutron stars in a binary system merge, the combined mass can exceed the limit for neutron star stability, leading to the formation of a black hole.\n",
            "\n",
            "5. **Primordial Black Holes**\n"
          ]
        }
      ],
      "source": [
        "system_message = \"\"\"\n",
        "The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\"\"\"\n",
        "\n",
        "user_message_1 = \"\"\"\n",
        "Hello, who are you?\n",
        "\"\"\"\n",
        "\n",
        "ai_message_1 = \"\"\"\n",
        "Greeting! I am an AI research assistant. How can I help you today?\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_message\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_message_1\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": ai_message_1\n",
        "\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "response = get_completion(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Generate an image of downtown Ho Chi Minh City\"\"\"\n",
        "\n",
        "message = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": prompt\n",
        "}]\n",
        "\n",
        "response = get_completion(message)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SUA2zIz5Tvo",
        "outputId": "5082ea91-5a01-4e2b-88ad-b1dee3230dec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm unable to generate images directly, but I can describe what a typical image of downtown Ho Chi Minh City might look like, so you can visualize or create it yourself.\n",
            "\n",
            "Imagine a bustling urban landscape with a mix of modern skyscrapers and historic French colonial buildings. The skyline is dominated by the Bitexco Financial Tower, a tall, sleek building with a helipad jutting out from its side. Nearby, you might see the Saigon Notre-Dame Basilica, a beautiful red-brick cathedral with twin bell towers, and the Saigon Central Post Office, an elegant yellow building designed by Gustave Eiffel.\n",
            "\n",
            "The streets are filled with a mix of cars, motorbikes, and pedestrians. Sidewalks are lined with street vendors selling everything from fresh fruit to traditional Vietnamese street food. There are also modern shopping malls, cafes, and restaurants, reflecting the city's rapid development.\n",
            "\n",
            "At night, the city lights up with neon signs and the glow from the windows of high-rise buildings, creating a vibrant and dynamic atmosphere. The Saigon River might be visible, with boats and ferries moving along its waters, adding to the lively scene.\n",
            "\n",
            "This description should give you a good starting point for visualizing or creating an image of downtown Ho Chi Minh City.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDwxYDOK6BKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "comet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "name": "prompt-engineering-part1.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}